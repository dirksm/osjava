<?xml version="1.0"?>
<document>

  <properties>
    <author email="bayard@generationjava.com">Henri Yandell</author>
    <title></title>
  </properties>

  <body>
  <section><p>
 Writing a scraper can be as simple as writing a parser and configuring the scraper.
Writing a parser

The parser, despite all the imports, is a pretty simple class. It's given a Page object, a Config and a Session. It's job is to return a Result.

Parsing code, if it uses the gj-scraper module, nearly always starts by loading the page into an HtmlScraper. From there the scraping is simple and a Result, in this case SingleResult, is created.


     

<pre>
package com.generationjava.comics;

import com.generationjava.scrape.HtmlScraper;
import com.generationjava.web.XmlW;
import java.io.IOException;

import java.net.URL;
import java.net.MalformedURLException;

import org.osjava.scraping.AbstractParser;
import com.generationjava.config.Config;
import org.osjava.scraping.Page;
import org.osjava.scraping.Session;
import org.osjava.scraping.ParsingException;
import org.osjava.scraping.Result;
import org.osjava.scraping.SingleResult;

public class UserFriendlyParser extends AbstractParser {

    public Result parse(Page page, Config cfg, Session session) throws ParsingException {
        HtmlScraper scraper = new HtmlScraper();
   
        try {
            scraper.scrape(page.readAsString());
        } catch(IOException ioe) {
            throw new ParsingException("Unable to read page. ", ioe);
        }
     
        scraper.moveToTagWith("ALT", "Latest Strip");
        String imgUrl = scraper.get("IMG<a href="SRC.html">SRC</a>");
   
        try {
            return new SingleResult( new URL(imgUrl) );
        } catch(MalformedURLException murle) {
            throw new ParsingException("Unable to parse url: " +imgUrl, murle);
        }
    }

}

</pre>
    

Configuring

Configuration happens through simple-jndi, meaning that you could put your configuration values in an LDAP server if you should so choose. At least you ought to be able to, it's untested. As well as ensuring you have a correctly set up log4j and jndi properties files, you will need a default.properties that looks something like:

      
     
<ol><li> first two lines configure scraping-engine under oscube</li></ol>
   
     
org.osjava.oscube.runner=org.osjava.scraping.ScrapingRunner
   
     
org.osjava.oscube.prefix=org.osjava.scrapers
   
     

<ol><li> now we hook up a scraper with the given name</li></ol>
   
     
org.osjava.scrapers=UserFriendly
   
     

<ol><li> now we declare variables for the given scraper</li></ol>
   
     
UserFriendly.uri=http://www.userfriendly.org/static/
   
     
UserFriendly.parser=com.generationjava.comics.UserFriendlyParser
   
     
UserFriendly.store=File
   
     
UserFriendly.file.saveAs=UserFriendly.gif
   
     
UserFriendly.file.path=/tmp/comics/
   
     

   
    

  </p></section>
  </body>

</document>
